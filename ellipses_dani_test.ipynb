{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyright: reportPrivateImportUsage=false, reportGeneralTypeIssues=false\n",
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "# import hydra\n",
    "# import hydra.core.hydra_config\n",
    "# import omegaconf\n",
    "\n",
    "# from analytic_svd_model import AnalyticSVDModel\n",
    "\n",
    "# #Register additional resolver for log path\n",
    "# omegaconf.OmegaConf.register_new_resolver(\"list_to_string\", lambda o: functools.reduce(lambda acc, x: acc+\", \"+x.replace(\"\\\"\",\"\").replace(\"/\",\" \"), o, \"\")[2:])\n",
    "# omegaconf.OmegaConf.register_new_resolver(\"eval\", lambda c: eval(c))\n",
    "\n",
    "import pytorch_lightning\n",
    "import pytorch_lightning.accelerators\n",
    "import pytorch_lightning.callbacks\n",
    "import pytorch_lightning.loggers\n",
    "import pytorch_lightning.utilities\n",
    "\n",
    "import torch\n",
    "import torch.utils.tensorboard\n",
    "import torch.version\n",
    "\n",
    "# from mnist_datamodule import MNISTDataModule\n",
    "from ellipses_datamodule import EllipsesDataModule\n",
    "# from learned_filter_model import LearnedFilterModel\n",
    "# from analytic_filter_model import AnalyticFilterModel\n",
    "# from learned_svd_model import LearnedSVDModel\n",
    "# from analytic_svd_model import AnalyticSVDModel\n",
    "\n",
    "import typing\n",
    "import sys\n",
    "check_python_version = (sys.version_info[0] >= 3) and (sys.version_info[1] >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model': {\n",
    "            'name': 'learned', \n",
    "            'initialization': 'zeros'}, \n",
    "        'dataset': {\n",
    "            'name': 'ellipses', \n",
    "            'img_size': 256, \n",
    "            'ellipse_count': 10, \n",
    "            'ellipse_size': 50, \n",
    "            'ellipse_size_min': 10, \n",
    "            'blurred': False}, \n",
    "        'out_dir': './LearnedRadonFilters', \n",
    "        'deterministic': True, \n",
    "        'seed': 1234567, \n",
    "        'device': 'cuda', \n",
    "        'num_workers': 4, \n",
    "        'checkpoint': None, \n",
    "        'validation_split_percent': 20, \n",
    "        'training_batch_size': 32, \n",
    "        'shuffle_training_data': True, \n",
    "        'drop_last_training_batch': False, \n",
    "        'validation_batch_size': 100, #'${training_batch_size}', \n",
    "        'shuffle_validation_data': False, \n",
    "        'drop_last_validation_batch': False, \n",
    "        'test_batch_size': 100, #'${training_batch_size}', \n",
    "        'shuffle_test_data': False, \n",
    "        'drop_last_test_batch': True, \n",
    "        'sino_angles': None, \n",
    "        'sino_positions': None, \n",
    "        'noise_level': 0.004, \n",
    "        'optimizer_lr': 0.05, \n",
    "        'epochs': 1, \n",
    "        'training_batch_count': -1, \n",
    "        'validation_batch_count': -1, \n",
    "        'test_batch_count': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('config.pkl', 'rb') as fp:\n",
    "    config_v2 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('config_09nov.pkl', 'rb') as fp:\n",
    "    config_v3 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'omegaconf.dictconfig.DictConfig'>\n"
     ]
    }
   ],
   "source": [
    "print(type(config))\n",
    "print(type(config_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'name': 'learned', 'initialization': 'zeros'}, 'dataset': {'name': 'ellipses', 'img_size': 256, 'ellipse_count': 10, 'ellipse_size': 50, 'ellipse_size_min': 10, 'blurred': False}, 'out_dir': './LearnedRadonFilters', 'deterministic': True, 'seed': 1234567, 'device': 'cuda', 'num_workers': 4, 'checkpoint': None, 'validation_split_percent': 20, 'training_batch_size': 32, 'shuffle_training_data': True, 'drop_last_training_batch': False, 'validation_batch_size': 100, 'shuffle_validation_data': False, 'drop_last_validation_batch': False, 'test_batch_size': 100, 'shuffle_test_data': False, 'drop_last_test_batch': True, 'sino_angles': None, 'sino_positions': None, 'noise_level': 0.004, 'optimizer_lr': 0.05, 'epochs': 1, 'training_batch_count': -1, 'validation_batch_count': -1, 'test_batch_count': -1}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(200 if self.config.test_batch_count == -1 else self.config.test_batch_count)*self.config.test_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, test_transform if self.config.dataset.blurred else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "32\n",
      "256\n",
      "10\n",
      "50\n",
      "10\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(config_v2.test_batch_count)\n",
    "print(config_v2.test_batch_size )\n",
    "\n",
    "print(config_v2.dataset.img_size )\n",
    "print(config_v2.dataset.ellipse_count)\n",
    "print(config_v2.dataset.ellipse_size)\n",
    "print(config_v2.dataset.ellipse_size_min)\n",
    "\n",
    "print(config_v2.dataset.blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_v2.test_batch_count = 1\n",
    "#config_v2.test_batch_size  = 1000\n",
    "\n",
    "config_v2.dataset.img_size  = 256 # 64 #256\n",
    "config_v2.dataset.ellipse_count = 1\n",
    "config_v2.dataset.ellipse_size = 40\n",
    "config_v2.dataset.ellipse_size_min = 40\n",
    "\n",
    "config_v2.dataset.blurred = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "config_v2_reduced_model = config_v2.copy()\n",
    "config_v2_reduced_model.dataset.img_size = 64\n",
    "print(config_v2_reduced_model.dataset.img_size)\n",
    "print(config_v2.dataset.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = EllipsesDataModule(config_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "cont = 0\n",
    "for X in datamodule.test_dataloader():\n",
    "    cont += 1\n",
    "    #all_images.append(X[0])\n",
    "\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cont = 0\n",
    "#for X in iter(datamodule.test_dataloader()):\n",
    "#    cont += 1\n",
    "\n",
    "#print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 64, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0].shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0][5].view(X[0][0].shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_img = X[0][7]\n",
    "type(my_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compose = []\n",
    "v_compose.append(torchvision.transforms.Resize(size=64))#config.resized_shape))\n",
    "\n",
    "if len(v_compose)==0:\n",
    "    dataset_transform = None\n",
    "else:\n",
    "    dataset_transform = torchvision.transforms.Compose(v_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n",
      "Compose(\n",
      "    Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
      ")\n",
      "torch.Size([32, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Resize funziona su un batch? --> Risposta: sì\n",
    "print(X[0].shape)\n",
    "print(dataset_transform)\n",
    "print(dataset_transform(X[0]).shape)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Test Ellipses')\n",
    "axs[0].imshow(X[0][2].view(X[0][2].shape[-2:]))\n",
    "axs[1].imshow(dataset_transform(X[0])[2].view((dataset_transform(X[0])[2]).shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compose = []\n",
    "v_compose.append(torchvision.transforms.Resize(size=256))#config.resized_shape))\n",
    "\n",
    "if len(v_compose)==0:\n",
    "    dataset_transform = None\n",
    "else:\n",
    "    dataset_transform = torchvision.transforms.Compose(v_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Seguito del precedente: ho ottenuto la stessa identica immagine? --> Risposta: sì\n",
    "print(all((X[0] == dataset_transform(X[0])).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compose = []\n",
    "v_compose.append(torchvision.transforms.Resize(size=64))#config.resized_shape))\n",
    "\n",
    "if len(v_compose)==0:\n",
    "    dataset_transform = None\n",
    "else:\n",
    "    dataset_transform = torchvision.transforms.Compose(v_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Che fa il modello? Lo verifico sui dati\n",
    "\n",
    "# 1) Carica il modello\n",
    "from learned_filter_model import LearnedFilterModel\n",
    "modelClass = LearnedFilterModel\n",
    "model = modelClass(config_v2)\n",
    "reduced_model = modelClass(config_v2_reduced_model)\n",
    "\n",
    "# 2) Usa i dati vecchi sul modello\n",
    "print(X[0].shape)\n",
    "output_model = model.test_step(batch=X, batch_idx=0)\n",
    "#print(output_model.shape)\n",
    "\n",
    "\n",
    "# 3) Usa i dati resized sul modello\n",
    "print(dataset_transform(X[0]).shape)\n",
    "output_model_resized = reduced_model.test_step(batch=[dataset_transform(X[0]), torch.arange(0,32,1)], batch_idx=0)\n",
    "#print(output_model_resized.shape)\n",
    "\n",
    "\n",
    "# 4) plotta i risultati (prossimi 2 blocchi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "<class 'dict'>\n",
      "dict_keys(['sinogram', 'noisy_sinogram', 'ground_truth', 'reconstruction'])\n",
      "torch.Size([32, 1, 256, 93])\n",
      "torch.Size([32, 1, 256, 93])\n",
      "torch.Size([32, 1, 64, 64])\n",
      "torch.Size([32, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(len(output_model_resized))\n",
    "print(type(output_model_resized))\n",
    "print(output_model_resized.keys())\n",
    "print(output_model_resized['sinogram'].shape)\n",
    "print(output_model_resized['noisy_sinogram'].shape)\n",
    "print(output_model_resized['ground_truth'].shape)\n",
    "print(output_model_resized['reconstruction'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "<class 'dict'>\n",
      "dict_keys(['sinogram', 'noisy_sinogram', 'ground_truth', 'reconstruction'])\n",
      "torch.Size([32, 1, 256, 365])\n",
      "torch.Size([32, 1, 256, 365])\n",
      "torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(len(output_model))\n",
    "print(type(output_model))\n",
    "print(output_model.keys())\n",
    "print(output_model['sinogram'].shape)\n",
    "print(output_model['noisy_sinogram'].shape)\n",
    "print(output_model['ground_truth'].shape)\n",
    "print(output_model['reconstruction'].shape)\n",
    "\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "img_idx = 6\n",
    "\n",
    "fig, axs = plt.subplots(2, 4)\n",
    "fig.suptitle('Test reconstruction on original data vs resized data')\n",
    "axs[0,0].imshow(output_model['sinogram'][img_idx].view((output_model['sinogram'][img_idx]).shape[-2:]))\n",
    "axs[0,1].imshow(output_model['noisy_sinogram'][img_idx].view((output_model['noisy_sinogram'][img_idx]).shape[-2:]))\n",
    "axs[1,0].imshow(output_model['ground_truth'][img_idx].view((output_model['ground_truth'][img_idx]).shape[-2:]))\n",
    "axs[1,1].imshow(output_model['reconstruction'][img_idx].detach().view((output_model['reconstruction'][img_idx]).shape[-2:]).numpy())# lui viene vuoto. Ci sta, il modello non ha ancora imparato nulla\n",
    "\n",
    "axs[0,2].imshow(output_model_resized['sinogram'][img_idx].view((output_model_resized['sinogram'][img_idx]).shape[-2:]))\n",
    "axs[0,3].imshow(output_model_resized['noisy_sinogram'][img_idx].view((output_model_resized['noisy_sinogram'][img_idx]).shape[-2:]))\n",
    "axs[1,2].imshow(output_model_resized['ground_truth'][img_idx].view((output_model_resized['ground_truth'][img_idx]).shape[-2:]))\n",
    "axs[1,3].imshow(output_model_resized['reconstruction'][img_idx].detach().view((output_model_resized['reconstruction'][img_idx]).shape[-2:]).numpy()) # lui viene vuoto. Ci sta, il modello non ha ancora imparato nulla\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(2, 2)\n",
    "#fig.suptitle('Test reconstruction on original data')\n",
    "#axs[0,0].imshow(output_model['sinogram'][img_idx].view((output_model['sinogram'][img_idx]).shape[-2:]))\n",
    "#axs[0,1].imshow(output_model['noisy_sinogram'][img_idx].view((output_model['noisy_sinogram'][img_idx]).shape[-2:]))\n",
    "#axs[1,0].imshow(output_model['ground_truth'][img_idx].view((output_model['ground_truth'][img_idx]).shape[-2:]))\n",
    "#axs[1,1].imshow(output_model['reconstruction'][img_idx].detach().view((output_model['reconstruction'][img_idx]).shape[-2:]).numpy())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_v3.validation_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_v3.resized_shape = 128 #512 #128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ellipses_datamodule import EllipsesDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = EllipsesDataModule(config_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=128, interpolation=bilinear, max_size=None, antialias=None)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.dataset_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel_selection_mode: noalpha\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=128, interpolation=bilinear, max_size=None, antialias=None)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.test_dataloader().dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(datamodule.test_dataloader()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel_selection_mode: noalpha\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "cont = 0\n",
    "for Y in datamodule.test_dataloader():\n",
    "    cont += 1\n",
    "    break\n",
    "    #all_images.append(X[0])\n",
    "\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train (batches, images per batch, total)       640, 32, 20480\n",
    "<br>test        200     32 =  6400\n",
    "<br>validation  160     32 =  5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datamodule.test_dataloader().dataset.transform(Y[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compose = []\n",
    "v_compose.append(torchvision.transforms.Resize(size=64)) # !!! IMPORTANT !!! comment this line if you want \"original\"\n",
    "\n",
    "dataset_transform_postprocessing = torchvision.transforms.Compose(v_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 64, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset_transform_postprocessing(Y[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Test Ellipses')\n",
    "axs[0].imshow(Y[0][2].view(Y[0][2].shape[-2:]))\n",
    "axs[1].imshow((dataset_transform_postprocessing(Y[0][2])).view((dataset_transform_postprocessing(Y[0][2])).shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y[0][6].view(Y[0][6].shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.histogram(\n",
      "hist=tensor([    0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0., 16384.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.]),\n",
      "bin_edges=tensor([0.5000, 0.5100, 0.5200, 0.5300, 0.5400, 0.5500, 0.5600, 0.5700, 0.5800,\n",
      "        0.5900, 0.6000, 0.6100, 0.6200, 0.6300, 0.6400, 0.6500, 0.6600, 0.6700,\n",
      "        0.6800, 0.6900, 0.7000, 0.7100, 0.7200, 0.7300, 0.7400, 0.7500, 0.7600,\n",
      "        0.7700, 0.7800, 0.7900, 0.8000, 0.8100, 0.8200, 0.8300, 0.8400, 0.8500,\n",
      "        0.8600, 0.8700, 0.8800, 0.8900, 0.9000, 0.9100, 0.9200, 0.9300, 0.9400,\n",
      "        0.9500, 0.9600, 0.9700, 0.9800, 0.9900, 1.0000, 1.0100, 1.0200, 1.0300,\n",
      "        1.0400, 1.0500, 1.0600, 1.0700, 1.0800, 1.0900, 1.1000, 1.1100, 1.1200,\n",
      "        1.1300, 1.1400, 1.1500, 1.1600, 1.1700, 1.1800, 1.1900, 1.2000, 1.2100,\n",
      "        1.2200, 1.2300, 1.2400, 1.2500, 1.2600, 1.2700, 1.2800, 1.2900, 1.3000,\n",
      "        1.3100, 1.3200, 1.3300, 1.3400, 1.3500, 1.3600, 1.3700, 1.3800, 1.3900,\n",
      "        1.4000, 1.4100, 1.4200, 1.4300, 1.4400, 1.4500, 1.4600, 1.4700, 1.4800,\n",
      "        1.4900, 1.5000]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.histogram(Y[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "test_config = omegaconf.OmegaConf.create(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config.test_batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = {'color':{'name': 'blue', 'fav': True}, 'give_number':5} \n",
    "my_omega_config = omegaconf.OmegaConf.create(my_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_omega_config():\n",
    "    def __init__(self, config: omegaconf.DictConfig) -> None:\n",
    "        self.config = config.copy()\n",
    "\n",
    "    def print_number(self):\n",
    "        print(self.config.give_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_class = test_omega_config(my_omega_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "10\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(my_omega_config.give_number)\n",
    "my_class.print_number()\n",
    "\n",
    "my_omega_config.give_number = 10\n",
    "\n",
    "print(my_omega_config.give_number)\n",
    "my_class.print_number()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = my_omega_config.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.give_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to check whether the dataloader is deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ellipses_datamodule import EllipsesDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('config_09nov.pkl', 'rb') as fp:\n",
    "    config_v3 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = EllipsesDataModule(config_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyright: reportGeneralTypeIssues=false\n",
    "\n",
    "from math import atan, cos, sin, tan\n",
    "import typing\n",
    "\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms\n",
    "import torchvision.transforms.functional\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "import matplotlib.patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "check_python_version = (sys.version_info[0] >= 3) and (sys.version_info[1] >= 10)\n",
    "\n",
    "\n",
    "class _my_EllipsesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_count: int, img_size: int, ellipses_count: int, ellipses_size: float, ellipses_size_min: float=1, transform: typing.Callable[[torch.Tensor],torch.Tensor]|None if check_python_version else typing.Union[typing.Callable[[torch.Tensor],torch.Tensor] , None] = None, number_ellipses_poisson=False, random_size=True, generator: torch.Generator|None if check_python_version else typing.Union[torch.Generator, None] =None):\n",
    "        self.img_count = img_count\n",
    "        self.img_size = img_size\n",
    "        if number_ellipses_poisson == True:\n",
    "            self.ellipses_count = torch.poisson(torch.full((img_count,), ellipses_count).to(torch.float32), generator=generator).to(torch.int32)\n",
    "        else:\n",
    "            self.ellipses_count = (torch.full((img_count,), ellipses_count).to(torch.float32)).to(torch.int32)\n",
    "        real_ellipses_count = self.ellipses_count.sum()\n",
    "        self.ellipse_angle = torch.rand((real_ellipses_count,), generator=generator)*360.0\n",
    "        self.ellipse_alpha = torch.rand((real_ellipses_count,), generator=generator)*0.9+0.1\n",
    "        self.ellipse_width_aa  = torch.rand((real_ellipses_count,), generator=generator)*max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "        self.ellipse_height_aa = torch.rand((real_ellipses_count,), generator=generator)*max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "        self.ellipse_x_raw = torch.rand((real_ellipses_count,), generator=generator)\n",
    "        self.ellipse_y_raw = torch.rand((real_ellipses_count,), generator=generator)\n",
    "\n",
    "        r = torch.rand((real_ellipses_count,), generator=generator)*0.3\n",
    "        alpha = torch.rand((real_ellipses_count,), generator=generator)*2.0*torch.pi\n",
    "        self.ellipse_x_raw = torch.cos(alpha)*r+0.5\n",
    "        self.ellipse_y_raw = torch.sin(alpha)*r+0.5\n",
    "        if random_size:\n",
    "            self.ellipse_width_aa  = torch.rand((real_ellipses_count,), generator=generator)*0.15*img_size+0.05*img_size#max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "            self.ellipse_height_aa = torch.rand((real_ellipses_count,), generator=generator)*0.15*img_size+0.05*img_size#max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "        else:\n",
    "            self.ellipse_width_aa  = torch.ones(real_ellipses_count)*ellipses_size #max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "            self.ellipse_height_aa = torch.ones(real_ellipses_count)*ellipses_size #max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "\n",
    "\n",
    "        #self.ellipses_count = torch.ones((img_count,), dtype=torch.int32)\n",
    "        #self.ellipse_width_aa  = torch.full((img_count,), img_size)#ellipses_size)\n",
    "        #self.ellipse_height_aa = torch.full((img_count,), img_size)#ellipses_size)\n",
    "        #self.ellipse_x_raw = torch.full((img_count,), 0.5)\n",
    "        #self.ellipse_y_raw = torch.full((img_count,), 0.5)\n",
    "        #self.ellipse_angle = torch.zeros((img_count,))\n",
    "        #self.ellipse_alpha = torch.ones((img_count,))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        #self.channel_selection_mode = \"alpha\"\n",
    "        #if self[0][0].sum() == self.img_size*self.img_size:\n",
    "        #    self.channel_selection_mode = \"noalpha\"\n",
    "        self.channel_selection_mode = \"noalpha\"\n",
    "\n",
    "        print(f'channel_selection_mode: {self.channel_selection_mode}')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.img_count\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:\n",
    "        fig = plt.figure(figsize=(self.img_size,self.img_size), dpi=1)\n",
    "        ax = fig.add_axes([0.0,0.0,1.0,1.0])\n",
    "        ellipse_func = lambda w, h, a, t: (w/2.0*cos(t)*cos(a)-h/2.0*sin(t)*sin(a), w/2.0*cos(t)*sin(a)+h/2.0*sin(t)*cos(a))\n",
    "        prev_ellipses_count = self.ellipses_count[:idx].sum()\n",
    "        for i in range(self.ellipses_count[idx]):\n",
    "            e_idx = prev_ellipses_count+i\n",
    "            args = (self.ellipse_width_aa[e_idx].item(), self.ellipse_height_aa[e_idx].item(), self.ellipse_angle[e_idx].item()/180.0*torch.pi)\n",
    "            if self.ellipse_angle[idx] == 0.0:\n",
    "                ellipse_width = self.ellipse_width_aa[e_idx]\n",
    "                ellipse_height = self.ellipse_height_aa[e_idx]\n",
    "            else:\n",
    "                t = atan(-self.ellipse_height_aa[e_idx].item()*tan(self.ellipse_angle[e_idx].item()/180.0*torch.pi)/self.ellipse_width_aa[e_idx].item())\n",
    "                ellipse_width = max(ellipse_func(*args, t)[0], ellipse_func(*args, t+torch.pi)[0])-min(ellipse_func(*args, t)[0], ellipse_func(*args, t+torch.pi)[0])\n",
    "                t = atan(self.ellipse_height_aa[e_idx].item()/(tan(self.ellipse_angle[e_idx].item()/180.0*torch.pi)*self.ellipse_width_aa[e_idx].item()))\n",
    "                ellipse_height = max(ellipse_func(*args, t)[1], ellipse_func(*args, t+torch.pi)[1])-min(ellipse_func(*args, t)[1], ellipse_func(*args, t+torch.pi)[1])\n",
    "            ellipse_x = ellipse_width/2.0+self.ellipse_x_raw[e_idx].item()*(self.img_size-ellipse_width)\n",
    "            ellipse_y = ellipse_height/2.0+self.ellipse_y_raw[e_idx].item()*(self.img_size-ellipse_height)\n",
    "            ellipse = matplotlib.patches.Ellipse(xy=[ellipse_x, ellipse_y], width=self.ellipse_width_aa[e_idx].item(), height=self.ellipse_height_aa[e_idx].item(), angle=self.ellipse_angle[e_idx].item())\n",
    "            ax.add_artist(ellipse)\n",
    "            ellipse.set_clip_box(ax.bbox)\n",
    "            ellipse.set_alpha(self.ellipse_alpha[e_idx].item())\n",
    "            ellipse.set_facecolor(\"black\")\n",
    "            ellipse.set_edgecolor(None)\n",
    "            ellipse.set_antialiased(False)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_xlim(0.0, self.img_size)\n",
    "        ax.set_ylim(0.0, self.img_size)\n",
    "        fig.add_axes(ax)\n",
    "        fig.canvas.draw()\n",
    "        img = torch.from_numpy(np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8).copy())\n",
    "        if self.channel_selection_mode == \"noalpha\":\n",
    "            img = 1.0-torch.swapaxes(img.reshape(self.img_size,self.img_size,4), 0, 2).to(torch.float32)[3:4]/255.0\n",
    "        elif self.channel_selection_mode == \"alpha\":\n",
    "            img = torch.swapaxes(img.reshape(self.img_size,self.img_size,4), 0, 2).to(torch.float32)[0:1]/255.0\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        if self.transform != None:\n",
    "            img = self.transform(img)\n",
    "        plt.close()\n",
    "        return img, 0\n",
    "\n",
    "#import random\n",
    "#def seed_worker(worker_id):\n",
    "#    try:\n",
    "#        worker_seed = torch.cuda.initial_seed() % 2**32\n",
    "#    except:\n",
    "#        worker_seed = torch.initial_seed() % 2**32\n",
    "\n",
    "#    np.random.seed(worker_seed)\n",
    "#    random.seed(worker_seed)\n",
    "\n",
    "class my_EllipsesDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, config: omegaconf.DictConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config.copy()\n",
    "        self.training_generator = torch.Generator()\n",
    "        #self.training_generator.manual_seed(1234567) #(torch.randint(0, 999_999_999_999_999, (1,)).item())\n",
    "        self.validation_generator = torch.Generator()\n",
    "        #self.validation_generator.manual_seed(1234568) #(torch.randint(0, 999_999_999_999_999, (1,)).item())\n",
    "        self.test_generator = torch.Generator()\n",
    "        #self.test_generator.manual_seed(1234569) #(torch.randint(0, 999_999_999_999_999, (1,)).item())\n",
    "\n",
    "        v_compose = []\n",
    "        if config.dataset.blurred:\n",
    "            # v_compose.append(torchvision.transforms.Lambda(lambda x: torchvision.transforms.functional.gaussian_blur(x, 5, 2.5))) # old line (can be deleted)\n",
    "            v_compose.append(torchvision.transforms.GaussianBlur(kernel_size=5, sigma=2.5))\n",
    "            \n",
    "        # v_compose.append(torchvision.transforms.Lambda(lambda x: torchvision.transforms.Resize(size=config.resized_shape)(x))) # old line (can be deleted)\n",
    "        v_compose.append(torchvision.transforms.Resize(size=config.resized_shape)) # !!! IMPORTANT !!! comment this line if you want \"original\"\n",
    "\n",
    "        if len(v_compose)==0:\n",
    "            self.dataset_transform = None\n",
    "        else:\n",
    "            self.dataset_transform = torchvision.transforms.Compose(v_compose)\n",
    "\n",
    "        #self.training_generator.manual_seed(1234567)\n",
    "        #self.training_dataset = _my_EllipsesDataset((640 if self.config.training_batch_count == -1 else self.config.training_batch_count)*self.config.training_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, self.dataset_transform, self.training_generator) # if self.config.dataset.blurred else None)\n",
    "        #self.validation_generator.manual_seed(1234568)\n",
    "        #self.validation_dataset = _my_EllipsesDataset((160 if self.config.validation_batch_count == -1 else self.config.validation_batch_count)*self.config.validation_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, self.dataset_transform, self.validation_generator) # if self.config.dataset.blurred else None)\n",
    "        self.test_generator.manual_seed(1234569)\n",
    "        self.test_dataset = _my_EllipsesDataset((200 if self.config.test_batch_count == -1 else self.config.test_batch_count)*self.config.test_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, self.dataset_transform, self.test_generator) # if self.config.dataset.blurred else None)\n",
    "        \n",
    "\n",
    "    #def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "    #    #self.training_generator.manual_seed(1234567)\n",
    "    #    #training_dataset = _my_EllipsesDataset((640 if self.config.training_batch_count == -1 else self.config.training_batch_count)*self.config.training_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, self.dataset_transform, self.training_generator) # if self.config.dataset.blurred else None)\n",
    "    #    return torch.utils.data.DataLoader(self.training_dataset, drop_last=self.config.drop_last_training_batch, batch_size=self.config.training_batch_size, shuffle=self.config.shuffle_training_data, num_workers=self.config.num_workers, generator=self.training_generator)#, worker_init_fn=seed_worker)\n",
    "    \n",
    "    #def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "    #    #self.validation_generator.manual_seed(1234568)\n",
    "    #    #validation_dataset = _my_EllipsesDataset((160 if self.config.validation_batch_count == -1 else self.config.validation_batch_count)*self.config.validation_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, self.dataset_transform, self.validation_generator) # if self.config.dataset.blurred else None)\n",
    "    #    return torch.utils.data.DataLoader(self.validation_dataset, drop_last=self.config.drop_last_validation_batch, batch_size=self.config.validation_batch_size, shuffle=self.config.shuffle_validation_data, num_workers=self.config.num_workers, generator=self.validation_generator)#, worker_init_fn=seed_worker)\n",
    "\n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        print(\"I'm in test_dataloader.\")\n",
    "        print(f\"self.config.dataset.img_size is {self.config.dataset.img_size}\")\n",
    "        #self.test_generator.manual_seed(1234569)\n",
    "        #test_dataset = _my_EllipsesDataset((200 if self.config.test_batch_count == -1 else self.config.test_batch_count)*self.config.test_batch_size, self.config.dataset.img_size, self.config.dataset.ellipse_count, self.config.dataset.ellipse_size, self.config.dataset.ellipse_size_min, self.dataset_transform, self.test_generator) # if self.config.dataset.blurred else None)\n",
    "        #batch_size=self.config.test_batch_size\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, drop_last=self.config.drop_last_test_batch, batch_size=1, shuffle=self.config.shuffle_test_data, num_workers=self.config.num_workers, generator=self.test_generator)#, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel_selection_mode: noalpha\n"
     ]
    }
   ],
   "source": [
    "my_datamodule_v0 = my_EllipsesDataModule(config_v3)\n",
    "\n",
    "#test_generator = torch.Generator()\n",
    "#test_generator.manual_seed(1234569) #(torch.randint(0, 999_999_999_999_999, (1,)).item())\n",
    "\n",
    "my_test_datamodule_v0 = torch.utils.data.DataLoader(my_datamodule_v0.test_dataset, batch_size=config_v3.test_batch_size, shuffle=config_v3.shuffle_test_data) #, num_workers=config_v3.num_workers)\n",
    "\n",
    "# torch.utils.data.DataLoader(my_datamodule_v0.test_dataset, drop_last=config_v3.drop_last_test_batch, batch_size=1, shuffle=config_v3.config.shuffle_test_data, num_workers=config_v3.num_workers, generator=test_generator)#, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_v3.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_v3.test_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cont = 0\n",
    "for Y in my_test_datamodule_v0:\n",
    "    cont += 1\n",
    "    break\n",
    "    #all_images.append(X[0])\n",
    "\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y[0][1].view(Y[0][1].shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datamodule_v0.test_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(my_datamodule_v0.test_dataset[0][0].view(my_datamodule_v0.test_dataset[0][0].shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_image = Y[0][1].view(Y[0][1].shape[-2:]).clone() # my_datamodule_v0.test_dataset[0][0].view(my_datamodule_v0.test_dataset[0][0].shape[-2:]).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Test Ellipses')\n",
    "axs[0].imshow(Y[0][1].view(Y[0][1].shape[-2:]))\n",
    "#axs[0].imshow(my_datamodule_v0.test_dataset[0][0].view(my_datamodule_v0.test_dataset[0][0].shape[-2:]))\n",
    "axs[1].imshow(aus_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset_transform(X[0]), torch.arange(0,32,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_idx = torch.arange(0,32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compose = []\n",
    "v_compose.append(torchvision.transforms.Resize(size=256))#config.resized_shape))\n",
    "\n",
    "if len(v_compose)==0:\n",
    "    dataset_transform = None\n",
    "else:\n",
    "    dataset_transform = torchvision.transforms.Compose(v_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_resized_img = dataset_transform(my_img)\n",
    "my_resized_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Test Ellipses')\n",
    "axs[0].imshow(my_img.view(my_img.shape[-2:]))\n",
    "axs[1].imshow(my_resized_img.view(my_resized_img.shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.suptitle('Test Ellipses')\n",
    "axs[0,0].imshow(X[0][61].view(X[0][0].shape[-2:]))\n",
    "axs[0,1].imshow(X[0][68].view(X[0][0].shape[-2:]))\n",
    "axs[1,0].imshow(X[0][5].view(X[0][0].shape[-2:]))\n",
    "axs[1,1].imshow(X[0][23].view(X[0][0].shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'name': 'learned', 'initialization': 'zeros'}, 'dataset': {'name': 'ellipses', 'img_size': 64, 'ellipse_count': 1, 'ellipse_size': 16, 'ellipse_size_min': 16, 'blurred': False}, 'out_dir': './LearnedRadonFilters', 'deterministic': True, 'seed': 1234567, 'device': 'cuda', 'num_workers': 4, 'checkpoint': None, 'validation_split_percent': 20, 'training_batch_size': 32, 'shuffle_training_data': True, 'drop_last_training_batch': False, 'validation_batch_size': '${training_batch_size}', 'shuffle_validation_data': False, 'drop_last_validation_batch': False, 'test_batch_size': 1000, 'shuffle_test_data': False, 'drop_last_test_batch': True, 'sino_angles': None, 'sino_positions': None, 'noise_level': 0.004, 'optimizer_lr': 0.05, 'epochs': 1, 'training_batch_count': -1, 'validation_batch_count': -1, 'test_batch_count': 1}\n"
     ]
    }
   ],
   "source": [
    "print(config_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.Resize(size=64)(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_v2.dataset.blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compose = []\n",
    "\n",
    "# v_compose.append(torchvision.transforms.Lambda(lambda x: torchvision.transforms.functional.gaussian_blur(x, 5, 2.5)))\n",
    "\n",
    "v_compose.append(torchvision.transforms.Lambda(lambda x: torchvision.transforms.Resize(size=64)(x)))\n",
    "\n",
    "test_transform = torchvision.transforms.Compose(v_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dataset.blurred = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Lambda()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyright: reportGeneralTypeIssues=false\n",
    "\n",
    "from math import atan, cos, sin, tan\n",
    "import typing\n",
    "\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms\n",
    "import torchvision.transforms.functional\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "import matplotlib.patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import sys\n",
    "check_python_version = (sys.version_info[0] >= 3) and (sys.version_info[1] >= 10)\n",
    "\n",
    "class _EllipsesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_count: int, img_size: int, ellipses_count: int, ellipses_size: float, ellipses_size_min: float=1, transform: typing.Callable[[torch.Tensor],torch.Tensor]|None if check_python_version else typing.Union[typing.Callable[[torch.Tensor],torch.Tensor] , None] = None, number_ellipses_poisson=False, random_size=False):\n",
    "        self.img_count = img_count\n",
    "        self.img_size = img_size\n",
    "        if number_ellipses_poisson == True:\n",
    "            self.ellipses_count = torch.poisson(torch.full((img_count,), ellipses_count).to(torch.float32)).to(torch.int32)\n",
    "        else:\n",
    "            self.ellipses_count = (torch.full((img_count,), ellipses_count).to(torch.float32)).to(torch.int32)\n",
    "        real_ellipses_count = self.ellipses_count.sum()\n",
    "        self.ellipse_angle = torch.rand((real_ellipses_count,))*360.0\n",
    "        self.ellipse_alpha = torch.rand((real_ellipses_count,))*0.9+0.1\n",
    "        self.ellipse_width_aa  = torch.rand((real_ellipses_count,))*max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "        self.ellipse_height_aa = torch.rand((real_ellipses_count,))*max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "        self.ellipse_x_raw = torch.rand((real_ellipses_count,))\n",
    "        self.ellipse_y_raw = torch.rand((real_ellipses_count,))\n",
    "\n",
    "        r = torch.rand((real_ellipses_count,))*0.3\n",
    "        alpha = torch.rand((real_ellipses_count,))*2.0*torch.pi\n",
    "        self.ellipse_x_raw = torch.cos(alpha)*r+0.5\n",
    "        self.ellipse_y_raw = torch.sin(alpha)*r+0.5\n",
    "        if random_size:\n",
    "            self.ellipse_width_aa  = torch.rand((real_ellipses_count,))*0.15*img_size+0.05*img_size#max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "            self.ellipse_height_aa = torch.rand((real_ellipses_count,))*0.15*img_size+0.05*img_size#max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "        else:\n",
    "            self.ellipse_width_aa  = torch.ones(real_ellipses_count)*ellipses_size #max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "            self.ellipse_height_aa = torch.ones(real_ellipses_count)*ellipses_size #max(0.0, ellipses_size-ellipses_size_min)+ellipses_size_min\n",
    "\n",
    "\n",
    "        #self.ellipses_count = torch.ones((img_count,), dtype=torch.int32)\n",
    "        #self.ellipse_width_aa  = torch.full((img_count,), img_size)#ellipses_size)\n",
    "        #self.ellipse_height_aa = torch.full((img_count,), img_size)#ellipses_size)\n",
    "        #self.ellipse_x_raw = torch.full((img_count,), 0.5)\n",
    "        #self.ellipse_y_raw = torch.full((img_count,), 0.5)\n",
    "        #self.ellipse_angle = torch.zeros((img_count,))\n",
    "        #self.ellipse_alpha = torch.ones((img_count,))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.channel_selection_mode = \"alpha\"\n",
    "        if self[0][0].sum() == self.img_size*self.img_size:\n",
    "            self.channel_selection_mode = \"noalpha\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.img_count\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:\n",
    "        fig = plt.figure(figsize=(self.img_size,self.img_size), dpi=1)\n",
    "        ax = fig.add_axes([0.0,0.0,1.0,1.0])\n",
    "        ellipse_func = lambda w, h, a, t: (w/2.0*cos(t)*cos(a)-h/2.0*sin(t)*sin(a), w/2.0*cos(t)*sin(a)+h/2.0*sin(t)*cos(a))\n",
    "        prev_ellipses_count = self.ellipses_count[:idx].sum()\n",
    "        for i in range(self.ellipses_count[idx]):\n",
    "            e_idx = prev_ellipses_count+i\n",
    "            args = (self.ellipse_width_aa[e_idx].item(), self.ellipse_height_aa[e_idx].item(), self.ellipse_angle[e_idx].item()/180.0*torch.pi)\n",
    "            if self.ellipse_angle[idx] == 0.0:\n",
    "                ellipse_width = self.ellipse_width_aa[e_idx]\n",
    "                ellipse_height = self.ellipse_height_aa[e_idx]\n",
    "            else:\n",
    "                t = atan(-self.ellipse_height_aa[e_idx].item()*tan(self.ellipse_angle[e_idx].item()/180.0*torch.pi)/self.ellipse_width_aa[e_idx].item())\n",
    "                ellipse_width = max(ellipse_func(*args, t)[0], ellipse_func(*args, t+torch.pi)[0])-min(ellipse_func(*args, t)[0], ellipse_func(*args, t+torch.pi)[0])\n",
    "                t = atan(self.ellipse_height_aa[e_idx].item()/(tan(self.ellipse_angle[e_idx].item()/180.0*torch.pi)*self.ellipse_width_aa[e_idx].item()))\n",
    "                ellipse_height = max(ellipse_func(*args, t)[1], ellipse_func(*args, t+torch.pi)[1])-min(ellipse_func(*args, t)[1], ellipse_func(*args, t+torch.pi)[1])\n",
    "            ellipse_x = ellipse_width/2.0+self.ellipse_x_raw[e_idx].item()*(self.img_size-ellipse_width)\n",
    "            ellipse_y = ellipse_height/2.0+self.ellipse_y_raw[e_idx].item()*(self.img_size-ellipse_height)\n",
    "            ellipse = matplotlib.patches.Ellipse(xy=[ellipse_x, ellipse_y], width=self.ellipse_width_aa[e_idx].item(), height=self.ellipse_height_aa[e_idx].item(), angle=self.ellipse_angle[e_idx].item())\n",
    "            ax.add_artist(ellipse)\n",
    "            ellipse.set_clip_box(ax.bbox)\n",
    "            ellipse.set_alpha(self.ellipse_alpha[e_idx].item())\n",
    "            ellipse.set_facecolor(\"black\")\n",
    "            ellipse.set_edgecolor(None)\n",
    "            ellipse.set_antialiased(False)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_xlim(0.0, self.img_size)\n",
    "        ax.set_ylim(0.0, self.img_size)\n",
    "        fig.add_axes(ax)\n",
    "        fig.canvas.draw()\n",
    "        img = torch.from_numpy(np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8).copy())\n",
    "        if self.channel_selection_mode == \"noalpha\":\n",
    "            img = 1.0-torch.swapaxes(img.reshape(self.img_size,self.img_size,4), 0, 2).to(torch.float32)[3:4]/255.0\n",
    "        elif self.channel_selection_mode == \"alpha\":\n",
    "            img = torch.swapaxes(img.reshape(self.img_size,self.img_size,4), 0, 2).to(torch.float32)[0:1]/255.0\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        plt.close()\n",
    "        if self.transform != None:\n",
    "            img = self.transform(img)\n",
    "        return img, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = _EllipsesDataset((200 if config.test_batch_count == -1 else config.test_batch_count)*config.test_batch_size, config.dataset.img_size, config.dataset.ellipse_count, config.dataset.ellipse_size, config.dataset.ellipse_size_min, test_transform if config.dataset.blurred else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.test_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[4][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.suptitle('Test Ellipses')\n",
    "axs[0,0].imshow(test_dataset[4][0].view(test_dataset[4][0].shape[-2:]))\n",
    "axs[0,1].imshow(test_dataset[68][0].view(test_dataset[4][0].shape[-2:]))\n",
    "axs[1,0].imshow(test_dataset[6][0].view(test_dataset[4][0].shape[-2:]))\n",
    "axs[1,1].imshow(test_dataset[23][0].view(test_dataset[4][0].shape[-2:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genero tutte le immagini che mi servono\n",
    "# Train\n",
    "\n",
    "# Test\n",
    "\n",
    "# Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le metto ognuna in in utils.Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('py396_2022': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "728da063adee70d344dc69477bf203d2b79ed062e98e9ab3ec51282dd2dfe849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
